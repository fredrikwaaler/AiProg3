Critic:
  #Learning rate
  learning_rate: 0.01

  #Discount Factor
  discount_factor: 0.99

  #Eligibility decay
  eli_decay: 0.85

  internal_dims: 0

Actor:
  #Learning rate
  learning_rate: 0.1

  #Discount Factor
  discount_factor: 0.9

  #Eligibility decay
  eli_decay: 0.85

  #Epsilon
  epsilon: 1

  #epsilon decay
  epsilon_decay: 0.99995

Environment:
  #Initial state
  initial_state: !!python/list [-0.6, 0] # list: position (usually in the range [-0.6, -0.4]), velocity

  #Step reward
  step_reward: 1

  #Final reward
  final_reward: 10

  #Loser penalty
  loser_penalty: -20

  # granularity : number of state representations (boxes) over the whole state space
  granularity: !!python/list [4, 4]

  # max number of steps
  max_steps: 1000

  pos_range: !!python/tuple [-1.21, 0.61]
  velocity_range: !!python/tuple [-0.071, 0.071]

  # overlap between position bins
  pos_overlap: 0.05

  # overlap between velocity bins
  velocity_overlap: 0.004

Training:
  #Number of training episodes
  number_of_episodes: 100

  #Episodes to visualize
  visualize_episodes: [20, 50, 75, 99]
