Critic:
  #Learning rate
  learning_rate: 0.0001

  #Discount Factor
  discount_factor: 0.9

  #Eligibility decay
  eli_decay: 0.85

  internal_dims: !!python/list [32]

Actor:
  #Learning rate
  learning_rate: 0.7

  #Discount Factor
  discount_factor: 0.9

  #Eligibility decay
  eli_decay: 0.85

  #Epsilon
  epsilon: 1

  #epsilon decay
  epsilon_decay: 0.99

Environment:
  #Initial state
  initial_state: !!python/list [-0.6, 0] # list: position (usually in the range [-0.6, -0.4]), velocity

  #Step reward
  step_reward: 1

  #Final reward
  final_reward: 10

  #Loser penalty
  loser_penalty: -20

  # granularity : number of state representations (boxes) over the whole state space
  granularity: !!python/list [10, 10]

  # max number of steps
  max_steps: 1000

  # overlap between position bins
  pos_overlap: 0.05

  # overlap between velocity bins
  velocity_overlap: 0.004

Training:
  #Number of training episodes
  number_of_episodes: 500
